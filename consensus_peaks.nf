include { generate_matrix } from "./generate_matrices"
include { extract_meta_from_anndata } from "./converters"


process extract_pval {
    tag "${id}"
    //publishDir "${params.outdir}/${id}"
    conda "/home/sabramov/miniconda3/envs/jupyterlab"
    label 'high_mem'

    input:
        tuple val(id), path(pvals_parquet), path(bed_file)

    output:
        tuple val(id), path(name)

    script:
    name = "${id}.neglog10_pvals.npy"
    """
    hotspot3-pvals \
        ${pvals_parquet} \
        ${bed_file} \
        ${name} \
        --chrom_sizes ${params.nuclear_chrom_sizes} \
        --format npy
    """
}


workflow extractMaxPvalue {
    println "Taking samples order from anndata input (generated by build_index.nf) in - params.index_anndata = ${params.index_anndata}"


    index_meta = Channel.of(params.index_anndata)
        | extract_meta_from_anndata // path(masterlist), path(samples_order), path(saf_masterlist)
        | map(it -> tuple(it[0], it[1]))

    params.template_run = "/net/seq/data2/projects/sabramov/SuperIndex/hotspot3/peak_calls.v23/"

    data = Channel.fromPath(params.samples_file)
        | splitCsv(header:true, sep:'\t')
        | map(row -> tuple(
                row.ag_id,
                file("${params.template_run}/${row.ag_id}/${row.ag_id}.pvals.parquet")
            )
        )
        | combine(index_meta)
        | map(it -> tuple(it[0], it[1], it[2]))
        | extract_pval
        | map(it -> tuple("neglog10_pvals", it[1]))
        | combine(index_meta.map(it -> it[1])) // prefix, pvals, samples_order
        | groupTuple(by: [0, 2]) // prefix, pvals, samples_order
        | generate_matrix
}


process core_set {

    conda params.conda
    label 'medmem'
    publishDir "${params.outdir}/core_sets/${params.grouping_column}.${fdr}"
    tag "${prefix}"


    input:
        tuple val(grouping_key), path(pvals), path(anndata), val(fdr)
    
    output:
        tuple val(grouping_key), val(fdr), path("${prefix}.core_set.bed"), path("${prefix}.core_set.npy"), path("${prefix}.saturation_curve.npy"), path("${prefix}.saturation_curve_core.npy"),  path("${prefix}.step_added.npy"), path("${prefix}.mcv_by_step_stats.npy")
    
    script:
    prefix = "${grouping_key}.fdr${fdr}"
    name = "${prefix}.core_set.bed"
    npy_indicator = "${prefix}.npy"
    """
    python3 $moduleDir/bin/core_sets/core_set.py \
        ${params.samples_file} \
        ${params.grouping_column} \
        '${grouping_key}' \
        ${anndata} \
        ${pvals} \
        ${fdr} \
        ${prefix} \
    """
}


workflow generateCoreSets {
    core_set_fdrs = Channel.from(0.1, 0.05, 0.01, 0.001, 0.0001)
    println "Using ${params.grouping_column} as grouping column"
    params.pvals_matrix = "${params.outdir}/raw_matrices/matrix.neglog10_pvals.npy"
    data = Channel.fromPath(params.samples_file)
        | splitCsv(header:true, sep:'\t')
        | map(row -> tuple(
                row[params.grouping_column]
            )
        )
        | unique()
        | map(it -> tuple(it[0], file(params.pvals_matrix), file(params.index_anndata)))
        | combine(core_set_fdrs) // grouping_key, pvals, anndata, core_set_fdr
        | core_set
        | collectFile (
            skip: 1,
            keepHeader: true,
            storeDir: "${params.outdir}/core_sets/",
        ) { it -> 
            def parentDir = "${params.outdir}/core_sets/${params.grouping_column}.${it[1]}"
            [ 
            "${params.grouping_column}.core_sets_meta.tsv", 
            "group_key\tfdr\tcore_set_bed\tcore_set_npy\tcore_set_size\tsatuaration_curve\tsaturation_curve_core\tstep_added\tmcv_by_step_stats\n${it[0]}\t${it[1]}\t${parentDir}/${it[2].name}\t${parentDir}/${it[3].name}\t${it[2].countLines() - 1}\t${parentDir}/${it[4].name}\t${parentDir}/${it[5].name}\t${parentDir}/${it[6].name}\t${parentDir}/${it[7].name}\n" 
            ] 
        }
}