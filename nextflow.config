manifest {
  author = "Jeff Vierstra & Sergey Abramov"
  mainScript = "main.nf"
  defaultBranch = "main"
  name = "dhs-index-pipeline"
  description = "Build a DHS matrices from BAM files and hotspots"
  version = "0.1"
  nextflowVersion = "22.04.0"
}

// Run with a stricter shell, so errors are caught earlier
process.shell = ['/bin/bash', '-ueo','pipefail' ]

params {
	includeConfig './params.config'
}

profiles {
	Altius {
		// Run on SLURM and load the appropriate modules	
		process {
			executor = "slurm"
			queue = "queue0,queue2,encode4,pool,bigmem"
			memory = { 5.GB * task.attempt }
			cache = "lenient"
			errorStrategy = { (task.exitStatus in 137..143) ? 'retry' : 'ignore' }
			maxRetries = 3
			withLabel: bigmem {
				cpus = 50
				memory = 1500.GB
				queue = 'bigmem'
			}
			
			withLabel: medmem {
				memory = { 50.GB * task.attempt }
			}

			withLabel: highmem {
				memory = { 400.GB * task.attempt }
			}
 		}
        conda.enabled = true
		
		executor {
			$slurm {
				queueSize = 800

			}
		}
			
		// Logging
		// trace.enabled = true
		// trace.file = "pipeline_trace.txt"
		// timeline.enabled = true
		// timeline.file = "pipeline_timeline.html"
		// report.enabled = true
		// report.file = "pipeline_report.html"
	}

        
    old_cluster {
        process {
            queue = "queue0,encode4,pool,bigmem,hpcz-2"
        }
    }

    new_cluster {
        process {
            queue = "hpcz-test,bigmem"
        }
    }
}
