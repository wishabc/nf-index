manifest {
  author = "Jeff Vierstra & Sergey Abramov"
  mainScript = "main.nf"
  defaultBranch = "main"
  name = "dhs-index-pipeline"
  description = "Build a DHS matrices from BAM files and hotspots"
  version = "0.1"
  nextflowVersion = "22.04.0"
}

// Run with a stricter shell, so errors are caught earlier
process.shell = ['/bin/bash', '-ueo','pipefail' ]

params {
	includeConfig './params.config'
}

profiles {
	Altius {
		// Run on SLURM and load the appropriate modules	
		process {
			executor = "slurm"
			queue = "queue0,queue2,encode4,pool,bigmem"
			memory = { 5.GB * task.attempt }
			cache = "lenient"
			errorStrategy = { (task.exitStatus == 137 || task.exitStatus == 143 || task.exitStatus == 141) ? 'retry' : 'terminate' }
			maxRetries = 3
			withLabel: bigmem {
				cpus = 50
				memory = 1500.GB
				queue = 'bigmem'
			}
			
			withLabel: medmem {
				memory = { 50.GB * task.attempt }
			}

            withLabel: model_testing {
				memory = { 100.GB + (150.GB * task.attempt) }
			}

			withLabel: highmem {
				memory = { 450.GB * task.attempt }
			}

			withLabel: gpu {
				clusterOptions = "--gres=gpu:t4:2"
				queue = "hpcz-1,hpcz-1-short"
				maxRetries = 1
				memory = 200.GB
			}
 		}
        conda.enabled = true
		
		executor {
			$slurm {
				queueSize = 800

			}
		}
			
		// Logging
		// trace.enabled = true
		// trace.file = "pipeline_trace.txt"
		// timeline.enabled = true
		// timeline.file = "pipeline_timeline.html"
		// report.enabled = true
		// report.file = "pipeline_report.html"
	}
}
